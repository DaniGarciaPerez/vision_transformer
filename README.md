## <p align="center"> ðŸš§ REPO IN PROGRESS ðŸš§ </p>

## <p align="center"> **Vision Tranformer (ViT) from scratch** </p>

## **Content**

[1. Overview](#overview)

[2. Structure of the project](#structure-of-the-project)

[3. Disclaimer](#disclaimer)

[4. Setup and Use](#setup-and-use)

[5. Datasets](#datasets)

[6. Notes](#notes)

[7. Collaborators](#collaborators)

## **Overview**

Hey, thank you for passing by! 

This is a repo to explore the implementation of the Vision Transformer architecture based on the paper ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale". Alexey Dosovitskiy et al. (2021)](https://arxiv.org/pdf/2010.11929). I tried to keep everything as much similar as stated in the paper. Regarding the last layer on the network, the Multi Layer Perceptron (MLP) layer used for clasification on the architecture, this implementation relies on Global Average Pooling (GAP) rather than the CLS token based implementation used in the original paper, although they have a similar performance if train with the right hyperparameters.

## **Structure of the project**

```bash

```

## **Disclaimer**

This project was developed using:

- python 3.12.
- python modules as described in requirements.txt


## **Setup and Use**


## **Datasets**

- CIFAR100
- ImageNet
- FashionMnist

## **Notes**


## **Collaborators**

- [Dani Garcia](mailto:danielgarciache@gmail.com)



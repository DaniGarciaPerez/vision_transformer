## <p align="center"> üöß REPO IN PROGRESS üöß </p>

## <p align="center"> **Vision Transformers (ViT) from scratch** üëÅÔ∏è </p>

Hey, thank you for passing by! 

This is a repo to explore the implementation from scratch of the Vision Transformer architecture based on the paper ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale". Alexey Dosovitskiy et al. (2021)](https://arxiv.org/pdf/2010.11929) using PyTorch. I tried to keep everything as much similar to the paper as possible. Regarding the last layer of the network (the MLP layer used for classification), this implementation relies on Global Average Pooling (GAP) rather than the CLS token based implementation used in the original paper, although they have a similar performance if train with the right hyperparameters.

## **Content**

[1. Structure of the project](#structure-of-the-project)

[2. Disclaimer](#disclaimer)

[3. Setup and Use](#setup-and-use)

[4. Datasets](#datasets)

[5. Notes](#notes)

[6. Collaborators](#collaborators)

## **Structure of the project**

```bash

```

## **Disclaimer**

This project was developed using:

- python 3.12.
- python modules as described in requirements.txt


## **Setup and Use**


## **Datasets**

- CIFAR100
- ImageNet
- FashionMnist

## **Notes**


## **Collaborators**

- [Dani Garcia](mailto:danielgarciache@gmail.com)



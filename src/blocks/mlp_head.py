"""
Author: Daniel Garcia

Description: This module implements a Multi-Head Attention mechanism,
a key component of the Transformer architecture.
"""

import torch
from torch import nn
